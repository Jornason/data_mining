Practical Machine Learningï¼šPeer Assignment
========================================================

 
## Clean the Dataset
```{r,warning=FALSE,tidy=TRUE}
data<-read.csv("F:/coursera/Data Science/peer assignment/data mining/pml-training.csv")
data<-data[,-c(grep("kurtosis",names(data)),grep("skewness",names(data)),grep("max",names(data)),grep("min",names(data)),grep("amplitude",names(data)),grep("var",names(data)),grep("avg",names(data)),grep("stddev",names(data)))]
data<-data[,-c(1:2,5:7)]

```

## Cross Validation 
I use 70% data as the training set,and the left for testing.
```{r,warning=FALSE,tidy=TRUE}
library(caret)
library(ggplot2)
inTrain <- createDataPartition(y=data$classe,
                              p=0.7, list=FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]

```

## Model Building
I use support vector machine to build the model.
```{r,tidy=TRUE,message=FALSE}
library(e1071)
modFit <- svm(classe~ .,data=training,cross=10,type="C-classification")
modFit

```

## Evaluate the Model
First,I calculate the in sample error:
```{r}
pred1 <- predict(modFit,training)
(A<-table(pred1,training$classe))
sum(diag(A))/sum(A)

```
We can see the accuracy is `r sum(diag(A))/sum(A)`.If the model isn't overfitting,the out of sample error will be little big than in sample,I think it will no less than 90%.  
Now,we calculator the out of sample error:

```{r}
pred2 <- predict(modFit,testing)
(B<-table(pred2,testing$classe))

sum(diag(B))/sum(B)

```

## predict 20 different test cases
```{r}
test<-read.csv("F:/coursera/Data Science/peer assignment/data mining/pml-testing.csv")
test<-test[,-c(grep("kurtosis",names(test)),grep("skewness",names(test)),grep("max",names(test)),grep("min",names(test)),grep("amplitude",names(test)),grep("var",names(test)),grep("avg",names(test)),grep("stddev",names(test)))]
test<-test[,-c(1:2,5:7)]

pred<-predict(modFit,test)
pred
```

